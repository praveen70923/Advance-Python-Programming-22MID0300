{
 "cells": [
  {
   "cell_type": "code",
   "id": "991165ad-960a-46b4-9b5b-de360f705b5a",
   "metadata": {},
    "source": [
    "\"\"\"\n",
    "Title:   Web Text Tokenizer using NLTK and Project Gutenberg\n",
    "Author:  Praveen Kumar G (22MID0300)\n",
    "Date:    July 23, 2025\n",
    "Purpose: To demonstrate how to fetch online text from Project Gutenberg using urllib,\n",
    "         and tokenize it using the NLTK (Natural Language Toolkit) package.\n",
    "         The program prints the first 200 tokens from the raw text content.\n",
    "\"\"\""
  ]

  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ce2f70-4ad9-42b1-8655-6243089d344f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing 'request' from urllib to fetch web content\n",
    "from urllib import request\n",
    "\n",
    "# Importing nltk and its word tokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Downloading the 'punkt' tokenizer model (only required once per environment)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db903664-4163-49c7-877e-89e2b688fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fetch the raw text from a Project Gutenberg eBook\n",
    "url = \"https://www.gutenberg.org/cache/epub/76503/pg76503.txt\"  # Link to the eBook text\n",
    "response = request.urlopen(url)                                # Sending HTTP request and storing response\n",
    "raw = response.read().decode('utf8')                           # Decoding the byte stream into UTF-8 text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe05d3c-c827-4526-9443-1d641118a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenize the fetched text into words and punctuation\n",
    "tokens = word_tokenize(raw)  # Using NLTK's tokenizer to split raw text into individual tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00096a5c-1c83-4c62-ba4e-8720050fb823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 200 tokens from the eBook:\n",
      "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'man', 'who', 'mastered', 'time', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org', '.', 'If', 'you', 'are', 'not', 'located', 'in', 'the', 'United', 'States', ',', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'eBook', '.', 'Title', ':', 'The', 'man', 'who', 'mastered', 'time', 'Author', ':', 'Ray', 'Cummings', 'Illustrator', ':', 'Ed', 'Valigursky', 'Release', 'date', ':', 'July', '14', ',', '2025', '[', 'eBook', '#', '76503', ']', 'Language', ':', 'English', 'Original', 'publication', ':', 'New', 'York', ',', 'NY', ':', 'Ace', 'Books', ',', '1929', 'Credits', ':', 'Greg', 'Weeks', ',', 'Paul', 'Ereaut', ',', 'Mary', 'Meehan', '&', 'the', 'Online', 'Distributed', 'Proofreading', 'Canada', 'Team', 'at', 'http', ':', '//www.pgdpcanada.net', '*', '*', '*', 'START', 'OF', 'THE', 'PROJECT', 'GUTENBERG', 'EBOOK', 'THE', 'MAN', 'WHO', 'MASTERED', 'TIME', '*', '*', '*', 'THE', 'MAN', 'WHO', 'MASTERED', 'TIME', 'RAY', 'CUMMINGS', 'ACE', 'BOOKS', 'A', 'Division', 'of', 'A', '.', 'A.', 'Wyn', ',', 'Inc.', '23', 'West', '47th', 'Street', ',']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Display the first 200 tokens to verify the output\n",
    "print(\"First 200 tokens from the eBook:\")\n",
    "print(tokens[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b195a-1b81-4fdc-a661-585dcc45bffb",
   "metadata": {},
   "source": [
    "###  Summary:\n",
    "- Used `urllib.request` to fetch raw text data from an online source (Project Gutenberg).\n",
    "- Used `nltk.tokenize.word_tokenize` to split the large text into individual tokens (words/punctuations).\n",
    "- Downloaded the `punkt` tokenizer model for tokenization.\n",
    "- Displayed the first 200 tokens as proof of successful processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f35226-94bd-41fc-88d2-8980d6166c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
